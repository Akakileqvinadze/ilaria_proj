import os
from typing import List, Dict, Any, Literal

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

import google.generativeai as genai
from dotenv import load_dotenv  # ğŸ†• áƒ”áƒ¡ áƒ“áƒáƒ•áƒáƒ›áƒáƒ¢áƒáƒ—

load_dotenv()  # ğŸ†• .env áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ


# -----------------------------
# áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ˜áƒ¡ áƒ›áƒ”áƒ¡áƒ˜áƒ¯áƒ”áƒ‘áƒ˜
# -----------------------------
ERROR_MESSAGES = {
    "empty_query": "áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ“áƒáƒ¬áƒ”áƒ áƒáƒ— áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ",
    "service_unavailable": "áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ˜ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ— áƒ›áƒ˜áƒ£áƒ¬áƒ•áƒ“áƒáƒ›áƒ”áƒšáƒ˜áƒ",
    "processing_error": "áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ¡áƒáƒ¡",
}


# -----------------------------
# áƒ¡áƒ¥áƒ”áƒ›áƒ”áƒ‘áƒ˜ (request / response)
# -----------------------------
class ConversationMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str


class ChatRequest(BaseModel):
    message: str
    conversation_history: List[ConversationMessage] = []


class Source(BaseModel):
    id: int
    content: str
    section: str


class ChatResponse(BaseModel):
    answer: str
    sources: List[Source]
    metadata: Dict[str, Any]


# -----------------------------
# RagService â€“ áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ AI áƒáƒáƒ¡áƒ£áƒ®áƒ˜ (Gemini)
# -----------------------------
class RagService:
    def __init__(self) -> None:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise RuntimeError("GEMINI_API_KEY áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ’áƒáƒœáƒ¡áƒáƒ–áƒ¦áƒ•áƒ áƒ£áƒšáƒ˜ .env áƒ¤áƒáƒ˜áƒšáƒ¨áƒ˜")

        genai.configure(api_key=api_key)

        # áƒ¨áƒ”áƒ’áƒ˜áƒ«áƒšáƒ˜áƒ áƒ¨áƒ”áƒªáƒ•áƒáƒšáƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ˜
        self.model = genai.GenerativeModel("gemini-2.5-flash")

    async def generate_response(
        self,
        query: str,
        history: List[Dict[str, str]],
    ) -> Dict[str, Any]:
        """
        history áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ:
        [
          {"role": "user" | "assistant", "content": "..."},
          ...
        ]
        """

        # áƒ•áƒáƒ¨áƒ”áƒœáƒ”áƒ‘áƒ— prompt-áƒ¡: áƒ›áƒ—áƒ”áƒšáƒ˜ áƒ˜áƒ¡áƒ¢áƒáƒ áƒ˜áƒ + áƒáƒ®áƒáƒšáƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
        history_text_lines: List[str] = []
        for msg in history:
            prefix = "User" if msg["role"] == "user" else "Assistant"
            history_text_lines.append(f"{prefix}: {msg['content']}")

        history_text = "\n".join(history_text_lines) if history_text_lines else "â€”"

        system_instruction = (
            "áƒ¨áƒ”áƒœ áƒ®áƒáƒ  áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ” AI áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜ Subconscious áƒáƒáƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡. "
            "áƒ£áƒáƒáƒ¡áƒ£áƒ®áƒ” áƒ›áƒ™áƒáƒ¤áƒ˜áƒáƒ“, áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒáƒ“ áƒ“áƒ áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒáƒšáƒ£áƒ áƒáƒ“ áƒ¡áƒáƒ¡áƒáƒ áƒ’áƒ”áƒ‘áƒšáƒáƒ“. "
            "áƒ—áƒ£ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒœáƒáƒ—áƒ”áƒšáƒ˜, áƒ¡áƒ—áƒ®áƒáƒ•áƒ˜ áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ”áƒšáƒ¡ áƒ“áƒáƒ–áƒ£áƒ¡áƒ¢áƒ”áƒ‘áƒáƒ¡."
        )

        full_prompt = (
            f"{system_instruction}\n\n"
            f"áƒ¡áƒáƒ£áƒ‘áƒ áƒ˜áƒ¡ áƒ˜áƒ¡áƒ¢áƒáƒ áƒ˜áƒ:\n{history_text}\n\n"
            f"áƒáƒ®áƒšáƒ áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒšáƒ˜áƒ¡ áƒáƒ®áƒáƒšáƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ:\nUser: {query}\n\n"
            "áƒ’áƒ—áƒ®áƒáƒ• áƒ“áƒ”áƒ¢áƒáƒšáƒ£áƒ áƒ˜ áƒ“áƒ áƒ’áƒáƒ¡áƒáƒ’áƒ”áƒ‘áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒáƒ“."
        )

        try:
            # Gemini-áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ«áƒáƒ®áƒ”áƒ‘áƒ (áƒ¡áƒ˜áƒœáƒ¥áƒ áƒáƒœáƒ£áƒšáƒ˜áƒ, áƒ›áƒáƒ’áƒ áƒáƒ› FastAPI áƒáƒ›áƒáƒ¡ áƒ˜áƒ¡áƒ”áƒ•áƒ” áƒ˜áƒ—áƒ›áƒ”áƒœáƒ¡
            # áƒ—áƒ£ áƒ’áƒ˜áƒœáƒ“áƒ, áƒ¨áƒ”áƒ›áƒ“áƒ’áƒáƒ›áƒ¨áƒ˜ ThreadPoolExecutor-áƒ˜áƒ— áƒ’áƒáƒ“áƒáƒ•áƒ˜áƒ¢áƒáƒœáƒ—)
            response = self.model.generate_content(full_prompt)

            if not response or not response.text:
                answer_text = "áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ•áƒ«áƒ”áƒšáƒ˜ áƒáƒ› áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ, áƒ¡áƒªáƒáƒ“áƒ” áƒ¡áƒ®áƒ•áƒ áƒ¤áƒáƒ áƒ›áƒ£áƒšáƒ˜áƒ áƒ”áƒ‘áƒ."
            else:
                answer_text = response.text

            # áƒáƒ¥ áƒ¨áƒ”áƒ’áƒ˜áƒ«áƒšáƒ˜áƒ áƒ“áƒáƒáƒ›áƒáƒ¢áƒ áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ áƒ¬áƒ§áƒáƒ áƒáƒ”áƒ‘áƒ˜áƒ¡ áƒšáƒáƒ’áƒ˜áƒ™áƒ (RAG + vector DB),
            # áƒ¯áƒ”áƒ áƒ¯áƒ”áƒ áƒáƒ‘áƒ˜áƒ— áƒ•áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ— áƒ“áƒ”áƒ¤áƒáƒšáƒ¢áƒ¡
            sources = [
                {
                    "content": "áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒ’áƒ”áƒœáƒ”áƒ áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ Gemini áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ— áƒ›áƒáƒªáƒ”áƒ›áƒ£áƒšáƒ˜ áƒ˜áƒ¡áƒ¢áƒáƒ áƒ˜áƒ˜áƒ¡ áƒ“áƒ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ˜áƒ¡ áƒ¡áƒáƒ¤áƒ£áƒ«áƒ•áƒ”áƒšáƒ–áƒ”.",
                    "section": "model: gemini-2.5-flash",
                }
            ]

            metadata = {
                "model": "gemini-2.5-flash",
                "has_history": bool(history),
            }

            return {
                "answer": answer_text,
                "source_documents": sources,
                "metadata": metadata,
            }

        except Exception as e:
            # áƒ”áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒáƒ•áƒ áƒ›áƒáƒ¦áƒšáƒ áƒ“áƒ áƒ“áƒáƒ˜áƒ­áƒ”áƒ áƒ¡ /api/chat
            raise RuntimeError(f"Gemini error: {e}") from e


# áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘áƒ— áƒáƒ•áƒáƒ’áƒáƒ— RagService
try:
    rag_service: RagService | None = RagService()
except Exception as e:
    print(f"âŒ RagService init error: {e}")
    rag_service = None


# -----------------------------
# FastAPI áƒáƒáƒ˜
# -----------------------------
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # production-áƒ¨áƒ˜ áƒ¨áƒ”áƒ’áƒ˜áƒ«áƒšáƒ˜áƒ áƒ¨áƒ”áƒªáƒ•áƒáƒšáƒ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒ“áƒáƒ›áƒ”áƒœáƒ˜áƒ—
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    return {"status": "ok", "rag_service_ready": rag_service is not None}


@app.post("/api/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    # 1) áƒ•áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ—, áƒ“áƒáƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ“áƒ áƒ—áƒ£ áƒáƒ áƒ RagService
    if not rag_service:
        raise HTTPException(
            status_code=503,
            detail=ERROR_MESSAGES["service_unavailable"],
        )

    # 2) áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜ áƒ›áƒ”áƒ¡áƒ˜áƒ¯áƒ˜
    if not request.message or not request.message.strip():
        raise HTTPException(
            status_code=400,
            detail=ERROR_MESSAGES["empty_query"],
        )

    try:
        # 3) áƒ˜áƒ¡áƒ¢áƒáƒ áƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘áƒ RagService-áƒ¡áƒ—áƒ•áƒ˜áƒ¡
        history_payload = [
            {"role": msg.role, "content": msg.content}
            for msg in request.conversation_history
        ]

        # 4) áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜áƒ¡ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒªáƒ˜áƒ
        raw = await rag_service.generate_response(
            query=request.message,
            history=history_payload,
        )

        # 5) áƒ’áƒáƒ áƒ“áƒáƒ¥áƒ›áƒœáƒ ChatResponse áƒ¡áƒ¥áƒ”áƒ›áƒáƒ¨áƒ˜
        sources: List[Source] = []
        for i, doc in enumerate(raw.get("source_documents", [])):
            sources.append(
                Source(
                    id=i,
                    content=doc.get("content", ""),
                    section=doc.get("section", ""),
                )
            )

        response = ChatResponse(
            answer=raw.get("answer", ""),
            sources=sources,
            metadata=raw.get("metadata", {}),
        )
        return response

    except HTTPException:
        raise

    except Exception as e:
        print(f"âŒ Error in /api/chat: {e}")
        raise HTTPException(
            status_code=500,
            detail=ERROR_MESSAGES["processing_error"],
        )


# áƒ¡áƒ£áƒ áƒ•áƒ˜áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜, áƒ áƒáƒ› /chat-áƒ˜áƒª áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ“áƒ”áƒ¡ (áƒ¤áƒ áƒáƒœáƒ¢áƒ˜áƒ“áƒáƒœ áƒ—áƒ£ áƒ”áƒ’ áƒ’áƒ–áƒ áƒ›áƒáƒ“áƒ˜áƒ¡)
@app.post("/chat", response_model=ChatResponse)
async def chat_alias(request: ChatRequest):
    return await chat(request)
